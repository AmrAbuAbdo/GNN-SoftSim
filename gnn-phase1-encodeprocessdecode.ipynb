{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Thesis-gnn-phase1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECdzhRxjcXt3"
      },
      "source": [
        "# Graph Failure Propagation using Deepmind's graph_nets library\n",
        "\n",
        "This notebook models a network failure propagation scenario using a Graph Neural Network.\n",
        "\n",
        "The framework used is [Google's Deepmind GraphNet Framework](https://github.com/deepmind/graph_nets/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvW7y3lQX9Hr"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38W3iJmoqFtz"
      },
      "source": [
        "!df -h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMRTgD_1qMP0"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6O-lzZqSXa"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYLeRinwsMDt"
      },
      "source": [
        "%pip install --quiet watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEPjTqWTb22V"
      },
      "source": [
        "%pip install --quiet \"graph_nets>=1.1\" \"dm-sonnet>=2.0.0b0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zR-BuCrsVI5"
      },
      "source": [
        "%load_ext watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hcbwGRbsQa_"
      },
      "source": [
        "%watermark -u -n -t -z -p numpy,tensorflow,sonnet,graph_nets -g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKDbD-gJTgrj"
      },
      "source": [
        "## Configure Environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzWcYNkfcnq-"
      },
      "source": [
        "#@title Imports\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "import tensorflow as tf\n",
        "import networkx as nx\n",
        "import sonnet as snt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "try:\n",
        "  import seaborn as sns\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  sns.reset_orig()\n",
        "\n",
        "from graph_nets import blocks\n",
        "from graph_nets import utils_tf\n",
        "from graph_nets import utils_np\n",
        "from graph_nets.demos_tf2 import models\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "72Il4Vslc_Gf"
      },
      "source": [
        "#@title Set Random seeds\n",
        "SEED = 42 #@param\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaxLLlWXlZMt"
      },
      "source": [
        "#@title Load Graph Visualization Functions\n",
        "\n",
        "def plot_graphs_tuple(graphs_tuple):\n",
        "  networkx_graphs = utils_np.graphs_tuple_to_networkxs(graphs_tuple)\n",
        "  num_graphs = len(networkx_graphs)\n",
        "  _, axes = plt.subplots(1, num_graphs, figsize=(5*num_graphs, 5))\n",
        "  if num_graphs == 1:\n",
        "    axes = axes,\n",
        "  for graph, ax in zip(networkx_graphs, axes):\n",
        "    plot_graph_networkx(graph, ax)\n",
        "\n",
        "\n",
        "def plot_graph_networkx(graph, ax, pos=None):\n",
        "  node_labels = {node: \"{:.2g}|{:.2g}\".format(*data[\"features\"])\n",
        "                 for node, data in graph.nodes(data=True)\n",
        "                 if data[\"features\"] is not None}\n",
        "  edge_labels = {(sender, receiver): \"{:.2g}|{:.2g}\".format(*data[\"features\"])\n",
        "                 for sender, receiver, data in graph.edges(data=True)\n",
        "                 if data[\"features\"] is not None}\n",
        "  global_label = (\"{:.3g}\".format(graph.graph[\"features\"][0])\n",
        "                  if graph.graph[\"features\"] is not None else None)\n",
        "  node_color_map = [\"r\" if np.argmax(data[\"features\"]) == 1. else 'g'\n",
        "                 for node, data in graph.nodes(data=True)]\n",
        "  edge_color_map = [\"r\" if np.argmax(data[\"features\"]) == 1. else 'g'\n",
        "                 for sender, receiver, data in graph.edges(data=True)]\n",
        "  \n",
        "  \n",
        "  if pos is None:\n",
        "    random_pos = nx.random_layout(graph, seed=42)\n",
        "    pos = nx.spring_layout(graph, pos=random_pos)\n",
        "\n",
        "  nx.draw_networkx(graph, pos, ax=ax, \n",
        "                   labels=node_labels,\n",
        "                   edge_color=edge_color_map,\n",
        "                   node_color=node_color_map)\n",
        "\n",
        "  if edge_labels:\n",
        "    nx.draw_networkx_edge_labels(graph, pos, edge_labels, ax=ax)\n",
        "\n",
        "  if global_label:\n",
        "    plt.text(0.05, 0.95, global_label, transform=ax.transAxes)\n",
        "\n",
        "  ax.yaxis.set_visible(False)\n",
        "  ax.xaxis.set_visible(False)\n",
        "  return pos\n",
        "\n",
        "\n",
        "def plot_compare_graphs(graphs_tuples, labels):\n",
        "  pos = None\n",
        "  num_graphs = len(graphs_tuples)\n",
        "  _, axes = plt.subplots(1, num_graphs, figsize=(50*num_graphs, 50))\n",
        "  if num_graphs == 1:\n",
        "    axes = axes,\n",
        "  pos = None\n",
        "  for name, graphs_tuple, ax in zip(labels, graphs_tuples, axes):\n",
        "    graph = utils_np.graphs_tuple_to_networkxs(graphs_tuple)[0]\n",
        "    pos = plot_graph_networkx(graph, ax, pos=pos)\n",
        "    ax.set_title(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzOq2wzCd3sG"
      },
      "source": [
        "#@title Create Graph Dict { form-width: \"30%\" }\n",
        "#@markdown number of features per node\n",
        "NODE_FEATURES_DIM = 2 #@param\n",
        "#@markdown number of features per edge\n",
        "EDGES_FEATURES_DIM = 2 #@param\n",
        "\n",
        "def create_graph_dict(nodes_n, edges_n):\n",
        "  nodes = np.ones((nodes_n, NODE_FEATURES_DIM), dtype=np.float32)\n",
        "  n = nodes_n\n",
        "  d = edges_n \n",
        "\n",
        "  # Nodes: set all the values to healthy \n",
        "  # - healthy:[1, 0]\n",
        "  # - faulty:[0, 1][1, 0]\n",
        "  nodes[:, 1] = 0. \n",
        "  nodes[0, :] = [0., 1.]  # flip first\n",
        "\n",
        "  # we don't have a global state (for now)\n",
        "  globals_ft = [0.0, 0.0]\n",
        "  \n",
        "  # Edges.\n",
        "  edges, senders, receivers = [], [], []\n",
        "\n",
        "  perm_edges = list(itertools.combinations(np.arange(nodes_n), 2))\n",
        "  idx = np.arange(len(perm_edges)) \n",
        "  np.random.shuffle(idx)\n",
        "\n",
        "  for ix in idx[:edges_n]:\n",
        "    # Left incoming edge.\n",
        "    edges.append([1., 0])\n",
        "    a,b = perm_edges[ix]\n",
        "    senders.append(a)\n",
        "    receivers.append(b)\n",
        "\n",
        "  return {\n",
        "      \"globals\": globals_ft,\n",
        "      \"nodes\": nodes,\n",
        "      \"edges\": edges,\n",
        "      \"receivers\": receivers,\n",
        "      \"senders\": senders\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fflJID-9pWJv"
      },
      "source": [
        "#@title Flip Functions\n",
        "#@markdown These functions propagate the failure across the Graph\n",
        "def flip_edges(graph):\n",
        "    nodes = graph.nodes.numpy()\n",
        "    edges = graph.edges.numpy()\n",
        "    senders = graph.senders.numpy()\n",
        "\n",
        "    bad_nodes = []\n",
        "    for i in range(len(nodes)):\n",
        "        if nodes[i][0] == 0: # is dead\n",
        "            bad_nodes.append(i)\n",
        "\n",
        "    for ix, sender in enumerate(senders):\n",
        "        if sender in bad_nodes:\n",
        "            edges[ix][:] = [0.0, 1.0] \n",
        "\n",
        "    return graph.replace(\n",
        "        nodes=tf.convert_to_tensor(nodes),\n",
        "        edges=tf.convert_to_tensor(edges)\n",
        "    )\n",
        "def flip_nodes(graph):\n",
        "    nodes = graph.nodes.numpy()\n",
        "    edges = graph.edges.numpy()\n",
        "    receivers = graph.receivers.numpy()\n",
        "\n",
        "    for ix in range(len(edges)):\n",
        "        if edges[ix][0] == 0: # is dead\n",
        "            node_idx = receivers[ix]\n",
        "            nodes[node_idx][:] = [0., 1.0] # dead\n",
        "\n",
        "    return graph.replace(\n",
        "        nodes=tf.convert_to_tensor(nodes),\n",
        "    ) \n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtpDaP9fc4Q0"
      },
      "source": [
        "# Simulator\n",
        "\n",
        "With this logic we can create our desired simulator that updates the graph as a failure propagation across the nodes at each timestep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XW_z-0eM6aZ"
      },
      "source": [
        "def simulator(initial_state, steps):\n",
        "    current_state = initial_state\n",
        "    records = [ copy.copy(initial_state) ]\n",
        "    for s in range(steps):\n",
        "        if s % 2 == 1:\n",
        "            current_state = flip_nodes(current_state)\n",
        "        else:\n",
        "            current_state = flip_edges(current_state)\n",
        "        # propagate node value to outgoing edges\n",
        "        records.append(copy.copy(current_state))\n",
        "    return records"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2qdxlP3wYHD"
      },
      "source": [
        "# Create the Model\n",
        "\n",
        "\n",
        "## Encode Process Decode\n",
        "\n",
        "We will use the EncodeProcessDecode module.\n",
        "\n",
        "How many iterations we want to do on the graph is determined by the number of iterations.\n",
        "\n",
        "Parameters are shared between the different processing steps.\n",
        "\n",
        "Encoder:\n",
        "\n",
        "A \"Core\" graph net, which performs `N` rounds of processing (message-passing)steps. \n",
        "\n",
        "The input to the Core is the concatenation of the Encoder's output and the previous output of the Core (labeled \"Hidden(t)\" below, where \"t\" is the processing step).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZNgMmG8TnnH"
      },
      "source": [
        "#@title EncodeProcessDecode module\n",
        "model = models.EncodeProcessDecode(edge_output_size=EDGES_FEATURES_DIM, \n",
        "                                   node_output_size=NODE_FEATURES_DIM, \n",
        "                                   global_output_size=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NEJ4EDRyUGG"
      },
      "source": [
        "# Loss Function\n",
        "The training loss is computed on the output of each processing step. \n",
        "\n",
        "The reason for this is to encourage the model to try to solve the problem in as\n",
        "few steps as possible. \n",
        "\n",
        "It also helps make the output of intermediate steps more interpretable\n",
        "\n",
        "The loss is computed only on the final processing step.\n",
        "\n",
        "For each node and edge we want to compute the classification error using the crossentropy loss \n",
        "\n",
        "$y log(p)+(1-y)log(1-p)$ \n",
        "\n",
        "The loss for each step is the sum of the losses for each node, plus the sum of the losses for each edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkKTAc51BVuy"
      },
      "source": [
        "#@title Loss Function\n",
        "\n",
        "cross_entropy = tf.losses.categorical_crossentropy\n",
        "\n",
        "def create_loss(target, predicted, **kwds):\n",
        "    \"\"\"\n",
        "    sum of the crosspentropy loss for the edges and the nodes\n",
        "\n",
        "    Args:\n",
        "        target: a `graphs.GraphsTuple` which contains the target as a graph.\n",
        "        predicted: a `list` of `graphs.GraphsTuple`s which contains the model\n",
        "            outputs for each processing step as graphs.\n",
        "\n",
        "    Returns:\n",
        "        A `list` of ops which are the loss for each processing step. \n",
        "    \"\"\"\n",
        "    # this is the list of the mean losses for each timestep\n",
        "    # + label smoothing to help the learning phase\n",
        "    losses = [ \n",
        "            tf.math.reduce_sum(cross_entropy(target.nodes, output.nodes, **kwds)) + \n",
        "            tf.math.reduce_sum(cross_entropy(target.edges, output.edges, **kwds))\n",
        "    for output in predicted]\n",
        "\n",
        "    return losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF7cYOlYVn2p"
      },
      "source": [
        "#@title Accuracy \n",
        "def compute_accuracy(target, output):\n",
        "  \"\"\"Calculate model accuracy.\n",
        "\n",
        "  Returns the number of correctly predicted nodes and the number\n",
        "  of predicted edges\n",
        "\n",
        "  Args:\n",
        "    target: A `graphs.GraphsTuple` that contains the target graph.\n",
        "    output: A `graphs.GraphsTuple` that contains the output graph.\n",
        "\n",
        "  Returns:\n",
        "    correct: A `float` fraction of correctly labeled nodes.\n",
        "    solved: A `float` fraction of graphs that are completely correctly labeled.\n",
        "  \"\"\"\n",
        "  tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
        "  odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
        "  cs = []\n",
        "  ss = []\n",
        "  for td, od in zip(tdds, odds):\n",
        "    xn = np.argmax(td[\"nodes\"], axis=-1)\n",
        "    yn = np.argmax(od[\"nodes\"], axis=-1)\n",
        "    \n",
        "    xe = np.argmax(td[\"edges\"], axis=-1)\n",
        "    ye = np.argmax(od[\"edges\"], axis=-1)\n",
        "    # if the train node value is equal to the true node value\n",
        "    # if the edge node value is equal to the true edge value\n",
        "    # correct edges and nodes\n",
        "    c = np.concatenate((xn == yn, xe == ye), axis=0)\n",
        "    # solved\n",
        "    s = np.all(c)\n",
        "    cs.append(c)\n",
        "    ss.append(s)\n",
        "  correct = np.mean(np.concatenate(cs, axis=0))\n",
        "  solved = np.mean(np.stack(ss))\n",
        "  return correct, solved\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h86y4alSt1qK"
      },
      "source": [
        "# should be correct and solved ( 1.0, 1.0)\n",
        "graph_tuple = utils_tf.data_dicts_to_graphs_tuple([create_graph_dict(5, 5)])\n",
        "\n",
        "input_graph_tuple = graph_tuple\n",
        "(input_graph_tuple.nodes.shape, input_graph_tuple.edges.shape)\n",
        "compute_accuracy(input_graph_tuple, input_graph_tuple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emOoD2iAN2Xd"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuW0BoFLzWx5"
      },
      "source": [
        "\n",
        "- **Ltr**: training loss\n",
        "- **Lge**: test/generalization loss\n",
        "      \n",
        "- **Ctr**: training fraction nodes/edges labeled correctly\n",
        "\n",
        "- **Str**: training fraction examples solved correctly\n",
        "\n",
        "- **Cge**: test/generalization fraction nodes/edges labeled correctly \n",
        "\n",
        "- **Sge**: test/generalization fraction examples solved correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_MoX6xfPIp1"
      },
      "source": [
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1BeUXjAO6yJ"
      },
      "source": [
        "#@title # Reset State \n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "gc.collect()\n",
        "\n",
        "NUM_PROCESSING_STEPS=10 #@param \n",
        "\n",
        "#@markdown # Data / training parameters\n",
        "num_training_iterations = 2000 #@param \n",
        "BATCH_SIZE_TRAIN = 32 #@param \n",
        "BATCH_SIZE_VAL = 100 #@param\n",
        "\n",
        "EDGES_OUTPUT_DIM = EDGES_FEATURES_DIM\n",
        "NODES_OUTPUT_DIM = NODE_FEATURES_DIM\n",
        "\n",
        "# Model\n",
        "model = models.EncodeProcessDecode(edge_output_size=EDGES_OUTPUT_DIM, \n",
        "                                   node_output_size=NODES_OUTPUT_DIM, \n",
        "                                   global_output_size=None)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_57cpAzA1Zk"
      },
      "source": [
        "#@title Setup Optimizer, GraphNetwork\n",
        "learning_rate = 1e-3 #@param\n",
        "optimizer = snt.optimizers.Adam(learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imXgeFr3Aqps"
      },
      "source": [
        "#@title Update Step, this function updates the weights of our model based on the loss\n",
        "\n",
        "def update_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = model(inputs, NUM_PROCESSING_STEPS)\n",
        "    list_loss = create_loss(targets, outputs, from_logits=True)\n",
        "    # sum the losses of each step and averages over the number of steps\n",
        "    total_loss = tf.math.reduce_sum(list_loss) / NUM_PROCESSING_STEPS \n",
        "\n",
        "  gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "  optimizer.apply(gradients, model.trainable_variables)\n",
        "  return outputs, total_loss, list_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mvF9RJV9M07"
      },
      "source": [
        "NUM_NODES = 5\n",
        "NUM_EDGES = 6\n",
        "\n",
        "def create_example(num_nodes, num_edges, timesteps=NUM_PROCESSING_STEPS):\n",
        "    g = create_graph_dict(num_nodes, num_edges)\n",
        "    gt = utils_tf.data_dicts_to_graphs_tuple([g])\n",
        "    \n",
        "    # simulate the failure propagation\n",
        "    g_list = simulator(gt, steps=timesteps)\n",
        "    gt_final_state = g_list[-1]\n",
        "    return gt, gt_final_state\n",
        "\n",
        "\n",
        "def gen_batch(batch_size):\n",
        "    # Compute a random graph. \n",
        "    # NOTE: We need a better heuristic to generate graphs that \n",
        "    #       represent interesting cases for our problem\n",
        "    inputs_batch = []\n",
        "    targets_batch = []\n",
        "\n",
        "    num_nodes = NUM_NODES\n",
        "    num_edges = NUM_EDGES\n",
        "    for i in range(batch_size):\n",
        "        g = create_graph_dict(num_nodes, num_edges)\n",
        "        gt = utils_tf.data_dicts_to_graphs_tuple([g])\n",
        "        \n",
        "        # simulate the failure propagation\n",
        "        gt_list = simulator(gt, steps=NUM_PROCESSING_STEPS)\n",
        "        inputs_batch.append(gt)\n",
        "        targets_batch.append(gt_list[-1])\n",
        "\n",
        "    inputs_tf_batch = utils_tf.concat(inputs_batch, axis=0)\n",
        "    targets_tf_batch = utils_tf.concat(targets_batch, axis=0)\n",
        "    return inputs_tf_batch, targets_tf_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD_deqCI9Q4V"
      },
      "source": [
        "input_batch, target_batch = gen_batch(BATCH_SIZE_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xqv-383LdOA"
      },
      "source": [
        "len(input_batch[0]), len(target_batch[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZcmqddHPv7S"
      },
      "source": [
        "\n",
        "# LOOP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oilFRDSAsJ1y"
      },
      "source": [
        "#@title init state\n",
        "last_iteration = 0\n",
        "logged_iterations = []\n",
        "losses_tr = []\n",
        "corrects_tr = []\n",
        "solveds_tr = []\n",
        "losses_ge = []\n",
        "corrects_ge = []\n",
        "solveds_ge = []\n",
        "\n",
        "log_every_seconds =  5#@param\n",
        "num_processing_steps_tr = 10 #@param\n",
        "#@title Compile single graph update step\n",
        "gt, gt_truth = create_example(NUM_NODES, NUM_EDGES)\n",
        "\n",
        "single_input_signature = [\n",
        "  utils_tf.specs_from_graphs_tuple(gt),\n",
        "  utils_tf.specs_from_graphs_tuple(gt_truth)\n",
        "]\n",
        "\n",
        "single_compiled_update_step = tf.function(update_step, \n",
        "                                          input_signature=single_input_signature)\n",
        "g_in, g_out = create_example(NUM_NODES, NUM_EDGES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9pOwvhLuio-"
      },
      "source": [
        "# # train for a bit\n",
        "# for i in range(10):\n",
        "#     print(i)\n",
        "#     steps, loss, list_loss = single_compiled_update_step(g_in, g_out)\n",
        "#     labels = [ f\"loss:{v.numpy()}\" for v in list_loss]\n",
        "#     plot_compare_graphs(steps, labels)\n",
        "#     print(compute_accuracy(g_out, steps[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrGXbAyBwe5-"
      },
      "source": [
        "#@title sample with 5 nodes and 5 edges\n",
        "sample_graph = create_graph_dict(5, 6)\n",
        "graph_tuple = utils_tf.data_dicts_to_graphs_tuple([sample_graph])\n",
        "## seems we have to flip always the first \n",
        "results = simulator(graph_tuple, steps=9)\n",
        "plot_compare_graphs(results, [ f\"step{i}\" for i in range(10) ])\n",
        "# results = simulator(results[-1], steps=4)\n",
        "# plot_compare_graphs(results, [ f\"step{i+5}\" for i in range(5) ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOcsvvwUpvZM"
      },
      "source": [
        "#@title Train Loop\n",
        "start_time = time.time()\n",
        "last_log_time = start_time\n",
        "for iteration in range(last_iteration, num_training_iterations):\n",
        "  last_iteration = iteration\n",
        "  n_nodes, n_edges = NUM_NODES + 30, NUM_EDGES + 40\n",
        "\n",
        "  # outputs_tr, loss_tr = compiled_update_step(inputs_tr, targets_tr)\n",
        "  g_in, g_out = create_example(n_nodes, n_edges)\n",
        "\n",
        "  outputs_tr, total_loss_tr, _ = single_compiled_update_step(g_in, g_out)\n",
        "\n",
        "  the_time = time.time()\n",
        "  elapsed_since_last_log = the_time - last_log_time\n",
        "\n",
        "  # keep looping \n",
        "  if elapsed_since_last_log <= log_every_seconds: continue\n",
        "  \n",
        "  # LOG \n",
        "  num_processing_steps_ge = num_processing_steps_tr\n",
        "  g_val_in, g_val_out = create_example(n_nodes+1, n_edges+2)\n",
        "  \n",
        "  last_log_time = the_time\n",
        "  outputs_val = model(g_val_in, num_processing_steps_ge)\n",
        "  loss_list = create_loss(g_val_out, outputs_val)\n",
        "  loss_ge = loss_list[-1]\n",
        "\n",
        "  # outputs_ge is a list of N * batch_size graphs\n",
        "  \n",
        "  if elapsed_since_last_log <= log_every_seconds:\n",
        "    plot_compare_graphs( [outputs_val[-1], g_val_out], \n",
        "                      [ \"Predicted\", \"Actual\"])\n",
        "  \n",
        "  # plot_compare_graphs( [outputs_val[-1], g_val_in], \n",
        "  #                     [ \"result\", \"ground truth\"])\n",
        "  \n",
        "  correct_tr, solved_tr = compute_accuracy(g_out, outputs_tr[-1])\n",
        "  correct_ge, solved_ge = compute_accuracy(g_val_out, outputs_val[-1])\n",
        "  \n",
        "  elapsed = time.time() - start_time\n",
        "  # train\n",
        "  losses_tr.append(total_loss_tr.numpy())\n",
        "  corrects_tr.append(correct_tr)\n",
        "  solveds_tr.append(solved_tr)\n",
        "  # val\n",
        "  losses_ge.append(loss_ge.numpy())\n",
        "  corrects_ge.append(correct_ge)\n",
        "  solveds_ge.append(solved_ge)\n",
        "\n",
        "  logged_iterations.append(iteration)\n",
        "  print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, \"\n",
        "          \"Str {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(\n",
        "              iteration, elapsed, total_loss_tr.numpy(), loss_ge.numpy(),\n",
        "              correct_tr, solved_tr, correct_ge, solved_ge))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfIniQR6PxQj"
      },
      "source": [
        "print(\"Training overall:\")\n",
        "print(\"Number of evaluated models: \"+str(len(corrects_tr)))\n",
        "print(corrects_tr)\n",
        "print(solveds_tr)\n",
        "average = sum(corrects_tr) / len(corrects_tr)\n",
        "print(\"The average of correct classified nodes (Training) \" + str(round(average, 2)))\n",
        "\n",
        "print(\"Generalization overall:\")\n",
        "print(\"Number of evaluated models: \"+str(len(corrects_ge)))\n",
        "print(corrects_ge)\n",
        "print(solveds_ge)\n",
        "average = sum(corrects_ge) / len(corrects_ge)\n",
        "print(\"The average of correct classified nodes (Generaliztion) \" + str(round(average, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOUu6DB4xcbG"
      },
      "source": [
        "## Manual Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjnb17Laxeym"
      },
      "source": [
        "# MAX_ITERATIONS = 10\n",
        "# for node_variance in range(1, 10):\n",
        "#     for edge_variance in range(1, 10):\n",
        "#         g_in, g_out = create_example(NUM_NODES + node_variance, NUM_EDGES + edge_variance)\n",
        "#         for timesteps in range(1, MAX_ITERATIONS+1):\n",
        "#             results = model(g_in, timesteps)\n",
        "#             correct, solved = compute_accuracy(results[-1], g_out)\n",
        "#             if correct and solved:\n",
        "#                 plot_compare_graphs([g_in, results[-1], g_out], [ \"input\", f\"Correct:{correct:.2f} Solved:{solved:.2f}\", \"target\"])\n",
        "#                 break\n",
        "#             if timesteps == MAX_ITERATIONS:\n",
        "#                 print(\"not solved\")\n",
        "#                 plot_compare_graphs([g_in, results[-1], g_out], [ \"input\", f\"Correct:{correct:.2f} Solved:{solved:.2f}\", \"target\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WPeH6N-gQFpV"
      },
      "source": [
        "accuracy_list=[]\n",
        "for i in range(1, 2):\n",
        "    g_in, g_out = create_example(500, 15625)\n",
        "    results = model(g_in, 5)\n",
        "    correct, solved = compute_accuracy(results[-1], g_out)\n",
        "    accuracy_list.append(correct)\n",
        "    plot_compare_graphs([g_in, results[-1], g_out], [ \"Prediction\", \"Actual\"])\n",
        "    if i > 900:\n",
        "      plot_compare_graphs([g_in, results[-1], g_out], [ \"Prediction\", \"Actual\"])\n",
        "\n",
        "print(\"1k nodes generalization:\")\n",
        "print(accuracy_list)\n",
        "average = sum(accuracy_list) / len(accuracy_list)\n",
        "print(\"The average of correct classified nodes/edges \" + str(round(average, 2)))         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvSXVoFPovLj"
      },
      "source": [
        "fig= plt.figure(figsize=(18,3))\n",
        "plt.plot(losses_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_K5peK018h"
      },
      "source": [
        "%watermark -d -t -n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}